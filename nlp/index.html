<!DOCTYPE html>
<html lang="en">
    <head>
        <title></title>
        <meta charset="utf-8">
        <meta name="viewport" content="width=device-width, initial-scale=1">
      <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.2.1/jquery.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js"></script>
        <style>
            #box
            {
                
                margin-left: auto;
                margin-right: auto;
                width:80%;
                margin-bottom: 20px;
                                                                                                        
            }
              /* Note: Try to remove the following lines to see the effect of CSS positioning */
            .affix
            {
                top: 0;
                width: 100%;
                                
            }

 
            .affix + #container1
            {
                padding-top:70px;
                top:100px;
                
  
            }
            footer 
            {
                background-color: #555;
                color: white;
                padding: 15px;
            }
            
        </style>
    </head>
    <body style="line-height:5px">
        <div class="container-fluid" style="background-color:#EEEEEE">
        <div class="row">
            <div class="col-sm-8 col-md-8 col-lg-10">
        <div class="jumbotron" style="margin-bottom:0px;padding-top:10px;height:80px; color:#009999">
            <h1><strong>EMNLP 2018</strong></h1>
        </div>
            </div>
          
            </div>
        </div>
    
        <div class="container-fluid text-center" id="container1" style="height:1000px;padding-top:10px;">
            <div class="row">
                
                    <div class="page-header">
  <h2 style="color:#000066"><b>HIGHLIGHTS OF EMNLP, 2018</b></h2>
                    </div>
                    <div id="box">
  <div id="myCarousel" class="carousel slide" data-ride="carousel" >
    <!-- Indicators -->
    <ol class="carousel-indicators">
      <li data-target="#myCarousel" data-slide-to="1" class="active"><!-- Wrapper for slides -->    </li>
    </ol>
    <div class="carousel-inner" style=" width:100%; height: 500px "> 
      
      <div class="item active">
        <img src="emnlp1.jpg" alt="new" style="width:100%;">
      </div>
    
      <div class="item">
        <img src="emnlp2.jpg" alt="old" style="width:100%;">
      </div>
        
        
      <div class="item">
        <img src="emnlp4.jpg" alt="old" style="width:100%;">
      </div>
  </div>

    <!-- Left and right controls -->
    <a class="left carousel-control" href="#myCarousel" data-slide="prev">
      <span class="glyphicon glyphicon-chevron-left"></span>
      <span class="sr-only">Previous</span>
    </a>
    <a class="right carousel-control" href="#myCarousel" data-slide="next">
      <span class="glyphicon glyphicon-chevron-right"></span>
      <span class="sr-only">Next</span>
    </a>
  </div>
                        
                    </div>
                    

                
            </div>
            <div class="row">
                    <div class="box">
                        <div class="container">
                        
                            <p><h3>Conference on <b>Empirical Methods in Natural Language Processing (EMNLP), 2018</b> was held in  Brussels, Belgium from October 31 to November 4, 2018. Total papers presented are 550. The conference handbook can be found <a href="https://drive.google.com/file/d/1Ya0F3QaMqfACS7sHc9_bcnSfSjhWpumX/view">here</a> and the proceedings can be found <a href="https://aclanthology.coli.uni-saarland.de/events/emnlp-2018#D18-1">here</a> <br>
The program consisted of two days of workshops and tutorials and three days of main conference.</h3><br>
<h2><b>TUTORIALS</b></h2>
<h3>The following tutorials were conducted on October 31, 2018:</h3>
 <ul class="list-group"><h4>
  <li class="list-group-item"><b>Joint models for NLP </b>by <b>Yue Zhang :</b> Joint models allow relevant tasks to share common information while avoiding error propagation in multi-stage pepelines.  This tutorial reviewed main approaches to joint modeling for both statistical and neural methods.</li>
  <li class="list-group-item"><b>Graph Formalisms for Meaning Representations</b> by <b>Adam Lopez</b> and <b>Sorcha Gilroy :</b> The main focus was on Hyperedge Replacement Languages, a context-free graph rewriting system. It is one of the most common graph formalism to be studies in NLP. The tutorial also covered Regular Graph Languages, a subfamily of HRL which are closed under intersection. </li>
  <li class="list-group-item"><b>Writing Code for NLP Research</b> by <b>Matt Gardner</b>, <b>Mark Neumann</b>, <b>Joel Grus</b>, and <b>Nicholas Lourie :</b> This tutorial shared the best practices for writing code for NLP research, drawing on the instructors' experience designing the recently-released AllenNLP toolkit, a PyTorch-based library for deep learning NLP research. This helped the participants to write research code in a way that facilitates fast prototyping, easy debugging and easy experimentation, regardless of what framework they use.</li>
  <li class="list-group-item"><b>Deep Latent Variable Models of Natural Language</b> by <b>Alexander Rush</b>, <b>Yoon Kim</b>, and <b>Sam Wiseman :</b> This tutorial will cover deep latent variable models both in the case where exact inference over the latent variables is tractable and when it is not. The challenges of applying these families of methods to NLP problems, the recent successes and best practices were also discussed. </li>
<li class="list-group-item"><b>Standardized Tests as benchmarks for Artificial Intelligence</b> by <b>Mrinmaya Sachan</b>, <b>Minjoon Seo</b>, <b>Hannaneh Hajishirzi</b>, and <b>Eric Xing :</b> In this tutorial, the standardized tests which have replaced the turing test as a driver for progress in AI, are categorised into two categories: open domain tests such as reading comprehensions and elementary school tests where the goal is to find the support for an answer from the student curriculum, and closed domain tests such as intermediate level math and science tests (algebra, geometry, Newtonian physics problems, etc.). Unlike open domain tests, closed domain tests require the system to have significant domain knowledge and reasoning capabilities.</li>
     <li class="list-group-item"><b>Deep Chit-Chat: Deep Learning for ChatBots</b> by <b>Wei Wu</b> and <b>Rui Yan :</b> The tutorial was based on the long-term efforts on building conversational models with deep learning approaches for chatbots with some specific information such as personas, styles, and emotions, etc. </li>
</h4></ul> 
                                <br>


<img src="workshop.jpeg" class="img-thumbnail"  style="width:50%">


<h2><b>WORKSHOPS</b></h2>
<h3>The following workshops were conducted on October 31 and November 1, 2018:</h3>

<ul class="list-group"><h4>
    <li class="list-group-item"><b>Workshop on Health Text Mining and Information Analysis : </b>It was the ninth international workshop which provided an interdisciplinary forum for researchers interested in automated processing of health documents.</li>
    <li class="list-group-item"><b>Workshop on Abusive Language Online : </b>It brought researchers of various disciplines together to discuss approaches to abusive language used online with governments in the social media platforms. </li>
    <li class="list-group-item"><b>Search-Oriented Conversational AI : </b>It brought together AI/Deep Learning specialists on one hand and search/IR specialists on the other hand to lay the ground for search-oriented conversational AI and establish future directions and collaborations
</li>
    <li class="list-group-item"><b>Workshop on Computational Research in Phonetics, Phonology, and Morphology : </b>It was provided a forum for exchanging news of recent research developments and other matters of interest in computational morphology and phonology.
</li>
    <li class="list-group-item"><b>Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis : </b>It brought together researchers in Computational Linguistics working on Subjectivity and Sentiment Analysis and researchers working on interdisciplinary aspects of affect computation from text.
</li>
    <li class="list-group-item"><b>Workshop on Social Media Mining for Health Applications Workshop and Shared Task : </b>It served as a platform to discuss novel approaches to text and data mining methods that are applicable to social media data and will prove invaluable for health monitoring and surveillance.
</li>
    <li class="list-group-item"><b>Large-scale Biomedical Semantic Indexing and Question Answering : </b>This workshop was about systems that use the diverse and voluminous information available online to respond directly to the information needs of biomedical scientists.
</li>
    <li class="list-group-item"><b>Analyzing and Interpreting Neural Networks for NLP : </b>This brought together people who are attempting to peek inside the neural network black box, taking inspiration from machine learning, psychology, linguistics and neuroscience.
</li>
    <li class="list-group-item"><b>Workshop on Fact Extraction and Verification : </b>This was the first workshop which brought together researchers working on various tasks related to fact extraction and verification and also hosts the FEVER Challenge, an information verification shared task.
</li>
    <li class="list-group-item"><b>Workshop on Argument Mining : </b>This was the 5th international workshop which provided a continuing forum to the last four years’ Argumentation Mining workshops at ACL and NAACL, the first research forum devoted to argumentation mining in all domains of discourse.
</li>
    <li class="list-group-item"><b>Workshop on Noisy User-generated Text :</b> It was the fourth workshop that focused on Natural Language Processing applied to noisy user-generated text, such as that found in social media, online reviews, crowdsourced data, web forums, clinical records and language learner essays
</li>
    <li class="list-group-item"><b>Workshop on Universal Dependencies : </b>This workshop was about universal dependencies and papers that dealt with cross-lingual perspective.</li></h4>
</ul>
          
<br>
<h2><b>KEYNOTE SPEAKERS</b></h2>
<h3>Some of the renowned keynote speakers who were present at the event are:</h3>
                            
<ul class="list-group"><h4>
  	<li class="list-group-item">
        <div class="media">
  			<div class="media-left media-middle"><img src="johanbos.jpg" class="media-object" style="width:100px"></div>
  			<div class="media-body"><h4 class="media-heading"><b>Johan Bos :</b></h4><p><b>Title: The Moment of Meaning and the Future of Computational Semantics</b><br>
In this talk the main focus was on what role computational semantics could play in future natural language processing applications (including fact checking and machine translation). The main ideas was exemplified by the parallel meaning bank, a new corpus comprising texts annotated with formal meaning representations for English, Dutch, German and Italian.</p></div>
		</div></li>
    
    <li class="list-group-item">
        <div class="media">
  			<div class="media-left media-middle"><img src="juliahirschberg.jpg" class="media-object" style="width:100px"></div>
  			<div class="media-body"><h4 class="media-heading"><b>Julia Hirschberg :</b></h4><p><b>Title: Truth or Lie? Spoken Indicators of Deception in Speech</b><br>
Detecting deception from various forms of human behavior is a longstanding research goal which is of considerable interest to the military, law enforcement, corporate security, social services and mental health workers. However, both humans and polygraphs are very poor at this task. So, in this talk, more accurate methods that are developed to detect deception automatically from spoken language were presented.</p></div>
		</div></li>
    
    
    <li class="list-group-item">
        <div class="media">
  			<div class="media-left media-middle"><img src="gideonmann.png" class="media-object" style="width:100px"></div>
  			<div class="media-body"><h4 class="media-heading"><b>Gideon Mann :</b></h4><p><b>Title: Understanding the News that Moves Markets</b><br>
This talk reviewed the ways in which language technology is enabling market participants to quickly understand and respond to major world events and breaking business news. It outlined the state of the art in applications of NLP to finance and highlighted open problems that are being addressed by this emerging research.
</p></div>
		</div></li>
                                </ul>
<br>

 
<h2><b>DATASETS</b></h2>
<h3>Many new tasks and datasets were presented at the conference, many of which propose more challenging settings.Some of these are:</h3>

<ul class="list-group"><h4>
    <li class="list-group-item"><b>Cloze style question answering :</b><a href="http://aclweb.org/anthology/D18-1257"> CLOTH</a> consists of 7,131 passages and 99,433 questions used in middle-school and high-school language exams. </li>
    <li class="list-group-item"><b>Automatic story generation from videos :</b><a href="http://aclweb.org/anthology/D18-1117"> VideoStory</a> is a new large-scale dataset for video description. It contains 20k social media videos amounting to 396 hours of video with 123k sentences, temporally aligned to the video.</li>
    <li class="list-group-item"><b>Grounded common sense inference :</b><a href="http://aclweb.org/anthology/D18-1009"> Swag</a> is a new dataset with 113k multiple choice questions about a rich spectrum of grounded  situations. The task of grounded commonsense inference, unifying natural language inference and commonsense reasoning can be achieved from this.</li>
    <li class="list-group-item"><b>Dataset for document grounded coversations :</b> The dataset by <a href="http://aclweb.org/anthology/D18-1076">Kangyan Zhou</a> contains 4112 conversations with an average of 21.43 turns per conversation. This not only provides a relevant chat history while generating responses but also provide a source of information that the models could use.
</li>
    <li class="list-group-item"><b>Sequential open-domain question answering :</b><a href="http://aclweb.org/anthology/D18-1134"> QBLink</a> is a new dataset of fully human-authored questions. It includes existing strong question answering frameworks with previous questions to improve the overall question-answering accuracy in open-domain question answering. The dataset is publically available <a href="http://sequential.qanta.org.">here</a></li>
    <li class="list-group-item"><b>Coreference resolution :</b><a href="http://aclweb.org/anthology/D18-1016"> PreCo</a> is a large-scale English dataset for coreference resolution. The large corpus contains 38K documents and 12.5M words which are mostly from the vocabulary of English-speaking  preschoolers. The dataset is publically available <a href="https://preschool-lab.github.io/PreCo/.">here</a></li>
    <li class="list-group-item"><b>Multimodal reading comprehension :</b><a href="http://aclweb.org/anthology/D18-1166"> RecipeQA</a> is a dataset for multimodal comprehension of cooking recipes. It comprises of approximately 20K instructional recipes with multiple modalities such as titles, descriptions and aligned set of images. The dataset is publically available <a href="http://hucvl.github.io/recipeqa.">here</a></li>
    <li class="list-group-item"><b>Rare word representation :</b><a href="http://aclweb.org/anthology/D18-1169"> CAmbridge Rare word Dataset (CARD-660)</a> is an expert-annotated word similarity dataset which provides a highly reliable, yet challenging, benchmark for rare word representation techniques.The dataset is publically available <a href="https://pilehvar.github.io/card-660/.">here</a>
</li></h4>
</ul>
          <br>
           <img src="languagemodel.png" class="img-thumbnail"  style="width:50%; height:50%">
      <h2><b>NATURAL LANGUAGE MODELS</b></h2>
<h3>Language models are becoming more commonplace in NLP and many papers investigated different architectures and properties of such models. Some of them are:</h3>

<ul class="list-group"><h4>
    <li class="list-group-item"><a href="http://aclweb.org/anthology/D18-1179">This</a> paper by Matthew E. Peters provides a detailed empirical study of how the choice of neural architecture (e.g. LSTM, CNN, or self attention) influences both end task accuracy and qualitative properties of the representations that are learned. They additionally show that the representations vary with network depth: morphological information is encoded at the word embedding layer; local syntax is captured at lower layers and longer-range semantics are encoded at the upper layers.</li>
    <li class="list-group-item"><a href="http://aclweb.org/anthology/D18-1458">This</a> paper by Gongbo Tang shows self-attentional networks and CNNs do not outperform RNNs in modeling subject-verb agreement over long distances and self-attentional networks perform distinctly better than RNNs and CNNs on word sense disambiguation.</li>
    <li class="list-group-item"><a href="http://aclweb.org/anthology/D18-1523">This</a> paper by Asaf Amrami replaces the ngram-based language model(LM) with a recurrent one. This allows to effectively query in a creative way, using dynamic symmetric patterns. The combination of the RNN-LM and the dynamic symmetric patterns results in strong substitute vectors for WSI, allowing to surpass the current state-of-the-art on the SemEval 2013 WSI shared task by a large margin.
    </li>
    <li class="list-group-item"><a href="http://aclweb.org/anthology/D18-1505">This</a> paper by Kalpesh Krishna analyzes the performance of different sentiment classification models on syntactically complex inputs like A-but-B sentences. Using contextualized ELMo embeddings instead of logic rules yields significantly better performance.
    </li>
    <li class="list-group-item">In the BlackBoxNLP workshop, <a href="http://aclweb.org/anthology/W18-5426">this</a> paper by Mario Giulianelli uses diagnostic classifiers to keep track and improve number agreement in language models.
    </li>
    <li class="list-group-item">In the same BlackBoxNLP workshop, <a href="http://aclweb.org/anthology/W18-5423">this</a> paper by Ethan Wilcox showed that RNN language models can represent filler-gap dependencies and learn a particular subset of restrictions known as island constraints.  
    </li>
   
    
    
    </h4>
</ul>
          
                    
            </div>
        </div>
    </div>
</div>
       
</body>
</html>
